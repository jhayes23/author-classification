{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "#Importing training data\n",
    "df = pd.read_csv('./train.csv')\n",
    "df.dropna(axis=0) #Drop rows with missing data\n",
    "df.set_index('id', inplace = True) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82598ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages required for pos_tag to work.\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084aec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD 3 NEW FEATURES: NUMBER OF ADJECTIVES, NUMBER OF NOUNS, NUMBER OF VERBS\n",
    "import re #Regexp\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "#Adding a helper function for preprocessing function. The count_tags counts the number of occurences of \n",
    "# adjectives, nouns, verbs for a given corpus\n",
    "def count_tags(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tags = pos_tag(tokens, tagset='universal')\n",
    "    adjectives = sum(1 for word, tag in tags if tag.startswith('ADJ'))\n",
    "    nouns = sum(1 for word, tag in tags if tag.startswith('NOUN'))\n",
    "    verbs = sum(1 for word, tag in tags if tag.startswith('VERB'))\n",
    "    \n",
    "    return adjectives,nouns,verbs\n",
    "\n",
    "#Preprocessing function, to make it easy to replicate on  submission data.\n",
    "#The function prepares the data for analysis by doing various types of data cleaning and feature extractions.\n",
    "#This function creates several columns \"Features\" by extracting features from the existing given text.\n",
    "#Features are used to help model a correlation between the input data and its label. \n",
    "def processing(df):\n",
    "    #lowering and removing punctuation\n",
    "    df['processed'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x.lower()))\n",
    "    \n",
    "    #numerical feature engineering\n",
    "    #total length of sentence\n",
    "    df['length'] = df['processed'].apply(lambda x: len(x))\n",
    "    #get number of words including stop words\n",
    "    df['words'] = df['processed'].apply(lambda x: len(x.split(' ')))\n",
    "    #get number of words that aren't stopwords\n",
    "    df['words_not_stopword'] = df['processed'].apply(lambda x: len([t for t in x.split(' ') if t not in stopWords]))\n",
    "    #get the average word length\n",
    "    df['avg_word_length'] = df['processed'].apply(lambda x: np.mean([len(t) for t in x.split(' ') if t not in stopWords]) if len([len(t) for t in x.split(' ') if t not in stopWords]) > 0 else 0)\n",
    "    #get the number of commas\n",
    "    df['commas'] = df['text'].apply(lambda x: x.count(','))\n",
    "    #get the number of adjectives, nouns, and verbs\n",
    "    df['num_adjs'], df['num_nouns'], df['num_verbs'] = zip(*df['processed'].apply(count_tags))\n",
    "\n",
    "    return(df)\n",
    "\n",
    "#The dataframe is then passed into the function and is preproccessed.\n",
    "df = processing(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a64892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#A list that contains the features that the model will use to be trained\n",
    "not_pos_features = [c for c in df.columns.values if c  not in ['id','text','author','num_adjs','num_nouns','num_verbs']]\n",
    "features = [c for c in df.columns.values if c  not in ['id','text','author']]\n",
    "#A sublist of features that exclude the processed column\n",
    "numeric_features = [c for c in df.columns.values if c  not in ['id','text','author','processed']]\n",
    "#The class labels\n",
    "target = 'author'\n",
    "\n",
    "#Splits the data into a train set and a test set. The train set will be used to train the model\n",
    "#And the test set will be used to calculate the model classifier's performance.\n",
    "#33% of the data will be used to test and the random state is used to reproduce the same results.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.33, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#TF-IDF stands for Term Frequency - Inverse Document Frequency and basically works by counting the frequency of \n",
    "#a word to measure its importance or weight. The more a frequent a word appears in the document, the more likely\n",
    "#it is to fit in that class. The IDF is used to minimize the weight of common words that don't add meaning to the\n",
    "#classification of the text.\n",
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='processed')),\n",
    "                ('tfidf', TfidfVectorizer( stop_words='english'))\n",
    "            ])\n",
    "\n",
    "text.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#The Pipeline has two components; a selector which uses the NumberSelector transformer. And the StandardScaler\n",
    "#which is a preprocessing step that standardizes the each key column. This operation is performed on all the columns\n",
    "#before being concatenated in the FeatureUnion.\n",
    "length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "length.fit_transform(X_train)\n",
    "\n",
    "words =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "words_not_stopword =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words_not_stopword')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "avg_word_length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='avg_word_length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "commas =  Pipeline([\n",
    "                ('selector', NumberSelector(key='commas')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "num_adjs = Pipeline([\n",
    "                ('selector', NumberSelector(key='num_adjs')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "num_nouns = Pipeline([\n",
    "                ('selector', NumberSelector(key='num_nouns')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "num_verbs = Pipeline([\n",
    "                ('selector', NumberSelector(key='num_verbs')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "#Joins all of the pipelines, merging all of the feature sets from different transformers\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_not_stopword', words_not_stopword),\n",
    "                      ('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas)])\n",
    "#Same as above except this includes add'l features:  Number of adjectives, Number of Nouns, Number of verbs\n",
    "pos_feats = FeatureUnion([('text', text), \n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_not_stopword', words_not_stopword),\n",
    "                      ('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas),\n",
    "                      ('num_adjs', num_adjs),\n",
    "                      ('num_nouns', num_nouns),\n",
    "                      ('num_verbs', num_verbs)])\n",
    "\n",
    "#showing that the entire union can also be handled as one giant pipline.\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c357b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "#Creates a pipeline that includes the features and the logisitic regression classifier.\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('logistic', LogisticRegression(random_state = 42,max_iter=1000)),\n",
    "])\n",
    "#The model is then trained/fit with the training data\n",
    "pipeline.fit(X_train[not_pos_features], y_train)\n",
    "#Now that the model has been given training data, it can be used to make informed predictions on the classification\n",
    "#of unseen data which it is tested upon.\n",
    "preds = pipeline.predict(X_test[not_pos_features])\n",
    "#How accurate the model was at correctly classifying unseen data\n",
    "np.mean(preds == y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de514a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A report showing the performance of the classifier\n",
    "#Compute precision, recall, F-measure and support for each class.\n",
    "#Precision: % of class x predictions that are actually class x\n",
    "#Recall: % of true class x items that are predicted as class x\n",
    "#F1-Score: A combined measure that assesses the P/R tradeoff is F-measure (weighted harmonic mean)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ac194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a pipeline that includes the features and the logisitic regression classifier.\n",
    "pos_pipeline = Pipeline([\n",
    "    ('features',pos_feats),\n",
    "    ('logistic', LogisticRegression(random_state = 42,max_iter=1000)),\n",
    "])\n",
    "#The model is then trained/fit with the training data\n",
    "pos_pipeline.fit(X_train, y_train)\n",
    "#Now that the model has been given training data, it can be used to make informed predictions on the classification\n",
    "#of unseen data which it is tested upon.\n",
    "pos_preds = pos_pipeline.predict(X_test)\n",
    "#A measure of how accurate the model was at correctly classifying unseen data. The new accuracy is lower which\n",
    "#may mean that the feature weights may need to be adjusted or there is no correlation between the new features\n",
    "# and the class labels.\n",
    "np.mean(pos_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e593ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A report showing the performance of the classifier with added features\n",
    "#Compute precision, recall, F-measure and support for each class.\n",
    "#Precision: % of class x predictions that are actually class x\n",
    "#Recall: % of true class x items that are predicted as class x\n",
    "#F1-Score: A combined measure that assesses the P/R tradeoff is F-measure (weighted harmonic mean)\n",
    "print(classification_report(y_test, pos_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may skip the CV & fine tuning part, if you like.\n",
    "pipeline.get_params().keys()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Hyperparameters that will be tested to see which will help the model perform better on generalized/unseen data\n",
    "hyperparameters = {\n",
    "    'logistic__C': [0.001, 0.01, 0.1, 1, 10],  # Specify other hyperparameters here\n",
    "    'logistic__class_weight': [None, 'balanced'],\n",
    "}\n",
    "#GridSearch uses the hyperparameters defined above to see which works at returning optized predictions.\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(X_train[not_pos_features], y_train)\n",
    "\n",
    "#refitting on entire training data using best settings\n",
    "clf.refit\n",
    "\n",
    "#Classifier is makes a new set of predictions with updated optimal hyperparameters.\n",
    "preds = clf.predict(X_test[not_pos_features])\n",
    "probs = clf.predict_proba(X_test[not_pos_features])\n",
    "#A measure of how accurate the model was at correctly classifying unseen data. The number has also increase\n",
    "#which indicates that the adjusted hyperparameters were useful.\n",
    "np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9925c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearch uses the hyperparameters defined above to see which works at returning optized predictions.\n",
    "pos_clf = GridSearchCV(pos_pipeline, hyperparameters, cv=5)\n",
    "# Fit and tune model\n",
    "pos_clf.fit(X_train, y_train)\n",
    "\n",
    "#refitting on entire training data using best settings\n",
    "pos_clf.refit\n",
    "#Classifier is makes a new set of predictions with updated optimal hyperparameters.\n",
    "pos_preds = pos_clf.predict(X_test)\n",
    "pos_probs = pos_clf.predict_proba(X_test)\n",
    "#A measure of how accurate the model was at correctly classifying unseen data. The number has also increase\n",
    "#which indicates that the adjusted hyperparameters were useful, but still lower than without the part of speech\n",
    "#feature additions.\n",
    "np.mean(pos_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657819fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pos_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7dafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads in a test file for add'l model testing\n",
    "submission = pd.read_csv('./test.csv')\n",
    "\n",
    "#preprocessing step. Prepares the test data by extracting features from text.\n",
    "submission = processing(submission)\n",
    "#model is making predictions on the unseen data with the optimized hyperparameters.\n",
    "predictions = clf.predict_proba(submission[not_pos_features])\n",
    "preds = pd.DataFrame(data=predictions, columns = clf.best_estimator_.named_steps['logistic'].classes_)\n",
    "\n",
    "#generating a submission file\n",
    "result = pd.concat([submission[['id']], preds], axis=1)\n",
    "result.set_index('id', inplace = True)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd9520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos tagged version: model is making predictions on the unseen data with the optimized hyperparameters. Overall\n",
    "#the added features decreased the models performance.\n",
    "pos_predictions = pos_clf.predict_proba(submission)\n",
    "pos_preds = pd.DataFrame(data=pos_predictions, columns = clf.best_estimator_.named_steps['logistic'].classes_)\n",
    "\n",
    "#generating a submission file\n",
    "pos_result = pd.concat([submission[['id']], pos_preds], axis=1)\n",
    "pos_result.set_index('id', inplace = True)\n",
    "pos_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b5dbd7",
   "metadata": {},
   "source": [
    "### Share three observations about how model performance changed after the addition of the three new features and why that might be.\n",
    "\n",
    "(E.g. Did precision decrease or increase, and why?)  \n",
    "\n",
    "The accuracy is lower: I think that this is a result of the features skewing the accuracy of the models predictions. Also the classes are likely to be unbalanced. \n",
    "\n",
    "The precision for MWS lowered: Since there was little to no correlation between the added vectors, it helped increased the chances of false positives in the denominator. The model was less strict.\n",
    "\n",
    "Recall for HPL increased: The model is more lenient in classifying the HPL class. The f eatures may be highly relevant to that class\n",
    "\n",
    "The F1 score for HPL increased by 0.01 and decreased for MWS by 0.01: A resulting change from what happened with recall and precision class measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f8bb97",
   "metadata": {},
   "source": [
    "# Homework 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Fold\n",
    "#GridSearch uses the hyperparameters defined above to see which works at returning optized predictions.\n",
    "cv2 = GridSearchCV(pos_pipeline, hyperparameters, cv=2)\n",
    "# Fit and tune model\n",
    "cv2.fit(X_train, y_train)\n",
    "\n",
    "#refitting on entire training data using best settings\n",
    "cv2.refit\n",
    "#Classifier is makes a new set of predictions with updated optimal hyperparameters.\n",
    "cv2_preds = cv2.predict(X_test)\n",
    "cv2_probs = cv2.predict_proba(X_test)\n",
    "#A measure of how accurate the model was at correctly classifying unseen data. The number has also increase\n",
    "#which indicates that the adjusted hyperparameters were useful, but still lower than without the part of speech\n",
    "#feature additions.\n",
    "np.mean(cv2_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7186806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 - Fold\n",
    "#GridSearch uses the hyperparameters defined above to see which works at returning optized predictions.\n",
    "cv10 = GridSearchCV(pos_pipeline, hyperparameters, cv=10)\n",
    "# Fit and tune model\n",
    "cv10.fit(X_train, y_train)\n",
    "\n",
    "#refitting on entire training data using best settings\n",
    "cv10.refit\n",
    "#Classifier is makes a new set of predictions with updated optimal hyperparameters.\n",
    "cv10_preds = cv10.predict(X_test)\n",
    "cv10_probs = cv10.predict_proba(X_test)\n",
    "#A measure of how accurate the model was at correctly classifying unseen data. The number has also increase\n",
    "#which indicates that the adjusted hyperparameters were useful, but still lower than without the part of speech\n",
    "#feature additions.\n",
    "np.mean(cv10_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20 - Fold\n",
    "#GridSearch uses the hyperparameters defined above to see which works at returning optized predictions.\n",
    "cv20 = GridSearchCV(pos_pipeline, hyperparameters, cv=20)\n",
    "# Fit and tune model\n",
    "cv20.fit(X_train, y_train)\n",
    "\n",
    "#refitting on entire training data using best settings\n",
    "cv20.refit\n",
    "#Classifier is makes a new set of predictions with updated optimal hyperparameters.\n",
    "cv20_preds = cv20.predict(X_test)\n",
    "cv20_probs = cv20.predict_proba(X_test)\n",
    "#A measure of how accurate the model was at correctly classifying unseen data. The number has also increase\n",
    "#which indicates that the adjusted hyperparameters were useful, but still lower than without the part of speech\n",
    "#feature additions.\n",
    "np.mean(cv20_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fa8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print 10 most important and least important features for each class along with weights for prev added features\n",
    "cv2_best_model = cv2.best_estimator_\n",
    "cv2_coef = cv2_best_model.named_steps['logistic'].coef_ #This is an array of the feature weights\n",
    "\n",
    "coefficients = pd.concat([pd.DataFrame(X_train.columns),pd.DataFrame(np.transpose(cv2_coef))], axis = 1)\n",
    "\n",
    "coefficients.columns = ['feature','EAP','HPL','MWS']\n",
    "replacement_words = text.named_steps['tfidf'].get_feature_names_out() #Words from processed text pipeline\n",
    "\n",
    "#Replaces NaN entries with words from the TDIF Vectorizor list.\n",
    "def replace_nan_with_word(row):\n",
    "    index = row.name \n",
    "    if pd.notna(row['feature']):\n",
    "        return row['feature']\n",
    "    else:\n",
    "        return replacement_words[index-8]\n",
    "    \n",
    "coefficients['feature'] = coefficients.apply(replace_nan_with_word, axis=1)\n",
    "\n",
    "print(\"Top 10 Most important and Least important features for EAP\")\n",
    "print(coefficients.sort_values(by=['EAP'], ascending=False).head(10),\"\\n\\n\",coefficients.sort_values(by=['EAP'], ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 Most important and Least important features for HPL\")\n",
    "print(coefficients.sort_values(by=['HPL'], ascending=False).head(10),\"\\n\\n\",coefficients.sort_values(by=['HPL'], ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 Most important and Least important features for MWS\")\n",
    "print(coefficients.sort_values(by=['MWS'], ascending=False).head(10),\"\\n\\n\",coefficients.sort_values(by=['MWS'], ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f266b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weights learned for Num adjs, num verbs, and num nouns\n",
    "coefficients.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21f5fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_preds\n",
    "results = test_data.copy()\n",
    "results['predicted_label'] = predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
